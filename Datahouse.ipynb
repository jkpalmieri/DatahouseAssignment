{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67fca067-e7b4-4a3a-a271-67f582d89a5c",
   "metadata": {},
   "source": [
    "## <center> Datahouse Take Home Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b499596e-4ef8-4382-8edd-f026e5138f86",
   "metadata": {},
   "source": [
    "## IMPORTANT NOTE:\n",
    "\n",
    "For this assessment, I'll be taking one input in the form of JSON and releasing one output in the form of JSON. My job is to formulate a \"compatibility score\" [0,1] for the applicants as I see fit.\n",
    "\n",
    "In the assignment we are only told that there is a single json input with a team and applicants, we are not told that we know the names of these delimeters or the order they will be presented in. Even more so, we have no information what information about these employees will be present. \n",
    "\n",
    "To simplify my solution while attempting to account for general scenarios, I will make the following assumptions:\n",
    "\n",
    "    1.) The json string will import with a \"team\" object first (exact name match) and a \"applicants\" object second (exact name match).\n",
    "\n",
    "    2.) Found within these objects will be another list of objects representing individuals belonging to the super-group.\n",
    "\n",
    "    3.) For each individual, there will be a \"name\" string and an \"attributes\" object filled with integer characteristics (on a 1-10 scale) that we are interested in observing.\n",
    "\n",
    "    4.) The \"attributes\" object will contain the same attributes and be in the same order between both the \"team\" and \"applicants\" objects.\n",
    "\n",
    "These assumptions allow for a concise solution while still being broad enough to incorporate any additional attributes the company decides to value in the future. I look forward to discussing these assumptions further with the team."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce65aac-61fc-473c-9b16-4076e1d91455",
   "metadata": {},
   "source": [
    "## 1.) Data Loading and Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61fbe9c9-0530-4c5c-b607-c0efe074abb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll begin with general imports for packages and libraries\n",
    "import pandas as pd\n",
    "import json \n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92fe6ae9-3c24-4b12-b841-0e2c5859670c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'team': [{'name': 'Eddie',\n",
       "   'attributes': {'intelligence': 1,\n",
       "    'strength': 5,\n",
       "    'endurance': 3,\n",
       "    'spicyFoodTolerance': 1}},\n",
       "  {'name': 'Will',\n",
       "   'attributes': {'intelligence': 9,\n",
       "    'strength': 4,\n",
       "    'endurance': 1,\n",
       "    'spicyFoodTolerance': 6}},\n",
       "  {'name': 'Mike',\n",
       "   'attributes': {'intelligence': 3,\n",
       "    'strength': 2,\n",
       "    'endurance': 9,\n",
       "    'spicyFoodTolerance': 5}}],\n",
       " 'applicants': [{'name': 'John',\n",
       "   'attributes': {'intelligence': 4,\n",
       "    'strength': 5,\n",
       "    'endurance': 2,\n",
       "    'spicyFoodTolerance': 1}},\n",
       "  {'name': 'Jane',\n",
       "   'attributes': {'intelligence': 7,\n",
       "    'strength': 4,\n",
       "    'endurance': 3,\n",
       "    'spicyFoodTolerance': 2}},\n",
       "  {'name': 'Joe',\n",
       "   'attributes': {'intelligence': 1,\n",
       "    'strength': 1,\n",
       "    'endurance': 1,\n",
       "    'spicyFoodTolerance': 10}}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, import the json file ASSUMING STORED ON DESKTOP\n",
    "\n",
    "# Get the path to your desktop\n",
    "desktop_path = os.path.join(os.path.expanduser('~'), 'Desktop')\n",
    "\n",
    "# Specify the filename\n",
    "file_name = \"DHInput.json\"\n",
    "\n",
    "# Combine the desktop path and filename\n",
    "file_path = os.path.join(desktop_path, file_name)\n",
    "\n",
    "# Open the file in read mode\n",
    "with open(file_path, \"r\") as json_file:\n",
    "    # Load the JSON data\n",
    "    input_data = json.load(json_file)\n",
    "\n",
    "# Print the JSON data\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcd69765-3dd6-42ee-b439-877e7922775d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    name  intelligence  strength  endurance  spicyFoodTolerance\n",
      "0  Eddie             1         5          3                   1\n",
      "1   Will             9         4          1                   6\n",
      "2   Mike             3         2          9                   5\n",
      "   name  intelligence  strength  endurance  spicyFoodTolerance\n",
      "0  John             4         5          2                   1\n",
      "1  Jane             7         4          3                   2\n",
      "2   Joe             1         1          1                  10\n"
     ]
    }
   ],
   "source": [
    "# Next, transition to separate DataFrames for transformations\n",
    "\n",
    "# First, the current employees\n",
    "employee_df = pd.json_normalize(input_data['team'])\n",
    "\n",
    "# Get the list of column names\n",
    "columns = employee_df.columns\n",
    "\n",
    "# The below section is to remove the \"attributes\" header - not necessary, just done for aesthetic nature + slimming the output frame\n",
    "# Iterate over each column name\n",
    "new_columns = {}\n",
    "for column in columns:\n",
    "    # Check if the column starts with \"attributes.\"\n",
    "    if column.startswith(\"attributes.\"):\n",
    "        # Extract the attribute name after \"attributes.\"\n",
    "        attribute_name = column.split(\".\")[1]\n",
    "        # Create a new column name without \"attributes.\"\n",
    "        new_columns[column] = attribute_name\n",
    "\n",
    "# Rename the columns\n",
    "employee_df = employee_df.rename(columns=new_columns)\n",
    "\n",
    "# Print to verify\n",
    "print(employee_df)\n",
    "\n",
    "# Next the applicants\n",
    "applicants_df = pd.json_normalize(input_data['applicants'])\n",
    "\n",
    "# Get the list of column names\n",
    "columns = applicants_df.columns\n",
    "\n",
    "# Iterate over each column name\n",
    "new_columns = {}\n",
    "for column in columns:\n",
    "    # Check if the column starts with \"attributes.\"\n",
    "    if column.startswith(\"attributes.\"):\n",
    "        # Extract the attribute name after \"attributes.\"\n",
    "        attribute_name = column.split(\".\")[1]\n",
    "        # Create a new column name without \"attributes.\"\n",
    "        new_columns[column] = attribute_name\n",
    "\n",
    "# Rename the columns\n",
    "applicants_df = applicants_df.rename(columns=new_columns)\n",
    "\n",
    "# Print to verify\n",
    "print(applicants_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a84a5d2-ee2d-4321-aeb6-07fa2909bc46",
   "metadata": {},
   "source": [
    "Now that the data is fully transferred and formatted, we should determine our scoring approach.\n",
    "\n",
    "Since I don't know how many attributes there are, I don't want to hardcode any specific names or weight one higher than the other based on the attributes in the sample input. However, I still believe weighting is necessary in this case as specific values of a job application are more important for compatibility than others - as demonstrated by the presence of a more quirky attribute in \"spicyFoodTolerance.\"\n",
    "\n",
    "Therefore, the best approach is to assume that they are placed in descending order of importance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91e0e78-7ce3-4727-b82d-b04024620dc2",
   "metadata": {},
   "source": [
    "## 2.) Methodology and Scoring:\n",
    "\n",
    "I will be weighting the categories in descending order based on there being \"n\" number of categories.\n",
    "\n",
    "For example, if there are 5 categories, the first will be weighted as 5, the next 4, and so on.\n",
    "\n",
    "Key Assumption:\n",
    "\n",
    "    1. We want to hire applicants that are most similar to the current team members\n",
    "\n",
    "To do this, I will find the \"average employee score\" after weighting and then compare the difference of the weighted applicant score. \n",
    "The closer they match, the closer to 1.0 their final score will be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb26672a-3854-4064-b8fe-b34af69cda3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attribute column names\n",
    "attribute_names = [col for col in employee_df.columns if col != \"name\"]\n",
    "\n",
    "# Calculate weights based on the number of attributes\n",
    "num_attributes = len(attribute_names)\n",
    "attribute_weights = {name: num_attributes - i for i, name in enumerate(attribute_names)}\n",
    "\n",
    "# Create a new DataFrame with the weighted values\n",
    "weighted_employees_df = employee_df.copy()\n",
    "for attribute, weight in attribute_weights.items():\n",
    "    weighted_employees_df[attribute] *= weight\n",
    "\n",
    "# Calculate the theoretical maximum score\n",
    "theoretical_max_score = sum(attribute_weights.values()) * 10\n",
    "\n",
    "# Calculate the individual scores for each employee\n",
    "individual_scores = weighted_employees_df[attribute_names].sum(axis=1) / theoretical_max_score\n",
    "\n",
    "# Add the individual scores to a new temp DataFrame\n",
    "scores_temp_df = pd.DataFrame()\n",
    "scores_temp_df['individual_score'] = individual_scores\n",
    "\n",
    "# Calculate the average employee score\n",
    "average_team_score = scores_temp_df['individual_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b87776e9-32d7-407e-957c-42378a0c9449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we need to calculate each applicant's score and find its similarity to the average employee score\n",
    "# We can reuse some variables since we assume the attributes are the same between the team and applicants\n",
    "\n",
    "# Create a new DataFrame with the weighted values\n",
    "weighted_applicants_df = applicants_df.copy()\n",
    "for attribute, weight in attribute_weights.items():\n",
    "    weighted_applicants_df[attribute] *= weight\n",
    "\n",
    "# Calculate the individual scores for each applicant\n",
    "individual_scores = weighted_applicants_df[attribute_names].sum(axis=1) / theoretical_max_score\n",
    "\n",
    "# Find how similar to the average employee each applicant is, since this is what we're prioritizing\n",
    "# We do this by finding how close to the maximum difference each applicant is then subtracting this \"% of max difference\" from \n",
    "# our highest potential candidate score of 1.0\n",
    "# We round the final result for brevity\n",
    "maximum_possible_difference = 1 - average_team_score\n",
    "\n",
    "final_applicant_scores = round(1 - abs(individual_scores - average_team_score)/maximum_possible_difference,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adbb0a5-8088-4b3f-a3f6-6fce594c0930",
   "metadata": {},
   "source": [
    "## 3.) Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a01c80ed-2474-4aa7-a59d-c866ca7a9fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"scoredApplicants\": [\n",
      "    {\n",
      "      \"name\": \"John\",\n",
      "      \"score\": 0.92\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Jane\",\n",
      "      \"score\": 0.88\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Joe\",\n",
      "      \"score\": 0.63\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# We now need to write our output to a list of dictionaries then dump it as a string.\n",
    "\n",
    "# Create a list of dictionaries for scored applicants\n",
    "scoredApplicants = []\n",
    "\n",
    "# Iterate over each row of the DataFrame\n",
    "for index, row in applicants_df.iterrows():\n",
    "    scoredApplicants.append({\"name\": row['name'], \"score\": final_applicant_scores[index]})\n",
    "\n",
    "# Convert the list of dictionaries to a JSON string\n",
    "output_data = json.dumps({\"scoredApplicants\": scoredApplicants}, indent=2)\n",
    "\n",
    "print(output_data)\n",
    "\n",
    "# The following is how to overwrite the initial input file. I have it commented out for further validation of editing the input file.\n",
    "# Its also commented out in non-multi-line format because in notebooks it will still print output if done in that format\n",
    "\n",
    "# \"\"\"\n",
    "# # Get the path to your desktop\n",
    "# desktop_path = os.path.join(os.path.expanduser('~'), 'Desktop')\n",
    "\n",
    "# # Combine the desktop path and filename\n",
    "# file_path = os.path.join(desktop_path, file_name)\n",
    "\n",
    "# # Write the JSON string to the file\n",
    "# with open(file_path, \"w\") as file:\n",
    "# file.write(output_data)\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca24d47-7726-4d3b-85f6-b10f0ee8e046",
   "metadata": {},
   "source": [
    "## 4.) Potential Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf64315-b63e-432d-b0c7-31c97ebee7d6",
   "metadata": {},
   "source": [
    "There is a massive laundry list that could be improved in my code. Below are just the thoughts that immediately come to mind:\n",
    "\n",
    "The primary thing would be removing the hardcoding of \"team\" and \"applicants\" and instead index through the JSON.\n",
    "I didn't do this because I thought it would help with readability in the end and aid in looking through the code.\n",
    "\n",
    "I also have a hunch that the employee scoring and averaging process could be more concise.\n",
    "\n",
    "In terms of methodology, a company might want to hire based on \"what they're missing,\" not on \"what they have.\" \n",
    "Therefore, I could've found \"gaps\" in the weighted categories and then searched for these gaps in applicants.\n",
    "Instead, I chose to use the similarity methodology since with a smaller, tight-knit community (which I assumed the workplace to be), \n",
    "you'd want people to get along the best and I figured similarity scoring would be the most optimal way to do this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
